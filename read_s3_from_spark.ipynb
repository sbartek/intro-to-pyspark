{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running pyspark\n",
    "\n",
    "I assume that you have installed `pyspak` somehow similar to the guide here.\n",
    "\n",
    "<http://bartek-blog.github.io/python/spark/pyspark/2019/03/27/how-to-install-pyspark.html>\n",
    "\n",
    "Then you should start `pyspark` with\n",
    "```\n",
    "pyspark --packages=org.apache.hadoop:hadoop-aws:2.7.3\n",
    "```\n",
    "\n",
    "or create spark session with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pyspark course\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.3\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "### Read aws configuration\n",
    "For more details how to configure AWS access see <http://bartek-blog.github.io/s3/cli/aws/python/boto3/2018/09/10/AWS-CLI-And-S3.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "aws_profile = \"myaws\"\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.expanduser(\"~/.aws/credentials\"))\n",
    "access_id = config.get(aws_profile, \"aws_access_key_id\") \n",
    "access_key = config.get(aws_profile, \"aws_secret_access_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configure hadoop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_conf.set(\"fs.s3n.awsAccessKeyId\", access_id)\n",
    "hadoop_conf.set(\"fs.s3n.awsSecretAccessKey\", access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.option(\"header\", \"true\").csv(\"s3n://bartek-ml-course/predict_future_sales/sales_train.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- date_block_num: string (nullable = true)\n",
      " |-- shop_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- item_price: string (nullable = true)\n",
      " |-- item_cnt_day: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "sdf.groupBy(\"date\").agg(F.sum(F.col('item_cnt_day')).alias(\"items\"))\\\n",
    "    .repartition(1)\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .parquet(\"s3n://bartek-ml-course/predict_future_sales-aggregations/daily-total-sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-03 00:12:40          0 _SUCCESS\r\n",
      "2019-05-03 00:12:37      10013 part-00000-2b45b7ef-9bc2-406e-acd2-51bf154aa7d9-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://bartek-ml-course/predict_future_sales-aggregations/daily-total-sales/ --profile=myaws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date| items|\n",
      "+----------+------+\n",
      "|16.02.2013|6643.0|\n",
      "|09.02.2014|4646.0|\n",
      "|01.09.2014|2887.0|\n",
      "|18.10.2014|5001.0|\n",
      "|27.06.2015|2563.0|\n",
      "|17.09.2015|1887.0|\n",
      "|29.04.2013|2771.0|\n",
      "|12.04.2013|3947.0|\n",
      "|18.09.2014|2441.0|\n",
      "|15.08.2015|2201.0|\n",
      "|28.10.2015|3593.0|\n",
      "|05.02.2013|3302.0|\n",
      "|21.09.2013|6698.0|\n",
      "|31.05.2014|5395.0|\n",
      "|02.11.2014|4390.0|\n",
      "|08.07.2015|1905.0|\n",
      "|13.09.2015|2660.0|\n",
      "|06.10.2015|1343.0|\n",
      "|13.06.2013|3399.0|\n",
      "|22.02.2014|8472.0|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"s3n://bartek-ml-course/predict_future_sales-aggregations/daily-total-sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
